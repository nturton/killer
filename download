#! /usr/bin/python3
import hashlib
import html.parser
import os
import os.path
import sys
import re
import requests

base_dir = os.path.dirname(sys.argv[0])

special_char_re = re.compile(r'[^\w\.]+')

site = "https://www.theguardian.com/lifeandstyle/"
killer_list_base = site + "series/killer-sudoku+series/sudoku?page="
ignore_re_list = (
  re.compile(r'[-\w]+'),
  re.compile(r'series/killer-sudoku\+series/sudoku\?page=\d+'),
  re.compile(r'series/killer-sudoku\+series/sudoku'),
)
puzzle_page_re_list = (
  re.compile(r'(\d+/[^\W\d]+/\d+)/observer-killer-sudoku'),
  re.compile(r'(\d+/[^\W\d]+/\d+)/sudoku-killer-\d+'),
)
image_ignore_re_list = (
  re.compile(r'//phar.gu-web.net/.*'),
  re.compile(r'https://sb.scorecardresearch.com/.*'),
)
puzzle_image_re_list = (
  re.compile(r'https://i\.guim\.co\.uk/img/static/sys-images/Guardian/Pix/pictures/'
             '(\d+/\d+/\d+)/[-/\w]+.jpeg\?.*'),
  re.compile(r'https://i\.guim\.co\.uk/img/media/\w+/[\w_]+/[\w]+/\d+.jpg?.*'),
)
extn_re = re.compile(r'.*(\.\w+)(\?|\Z)')

cache_dir = os.path.join(base_dir, "cache")

chunk_size = 4096

def url_cache_path(url):
  m = extn_re.match(url)
  extn = ""
  if m:
    extn = m.group(1)

  h = hashlib.sha256()
  h.update(url.encode("UTF-8"))
  dig = h.digest()
  return os.path.join(cache_dir, dig.hex()+extn)

def download_file(url):
  path = url_cache_path(url)
  tmp_file = path + ".tmp"

  print("Downloading %r" % (url,))

  if os.path.exists(path):
    print("  Local: %r (cached)" % (path,))
    return path

  f = open(tmp_file, "wb")
  r = requests.get(url, stream=True)
  for data in r.iter_content(chunk_size=chunk_size):
    f.write(data)
  f.close()
  os.rename(tmp_file, path)

  print("  Local: %r (fetched)" % (path,))
  return path

class link_handler(html.parser.HTMLParser):
  def __init__(self, tag, attr_name):
    super().__init__()
    self.__tag = tag
    self.__attr_name = attr_name
    self.__links = []

  def __iter__(self):
    return iter(self.__links)

  def handle_starttag(self, tag, attrs):
    if tag != self.__tag:
      return

    href = dict(attrs).get(self.__attr_name)
    if href == None:
      return

    self.__links.append(href)

def iter_elems(url, tag, attr_name):
  path = download_file(url)

  h = link_handler(tag, attr_name)
  f = open(path, "r")
  while True:
    data = f.read(chunk_size)
    if len(data) == 0:
      break
    h.feed(data)
  f.close()

  for link in h:
    yield link

def process_puzzle_page(info_set, url, puzzle_match):
  print("Puzzle page: %r" % (url,))

  the_puzzle = None
  for link in iter_elems(url, "img", "src"):
    should_ignore = False
    for r in image_ignore_re_list:
      if r.fullmatch(link):
        should_ignore = True
    if should_ignore:
      continue

    is_puzzle = False
    for r in puzzle_image_re_list:
      m = r.fullmatch(link)
      if m:
        is_puzzle = True

    if not is_puzzle:
      print("Unknown image on %r" % (url,))
      print("  Image: %r" % (link,))
      continue

    if the_puzzle != None:
      print("Duplicate puzzle found on %r" % (url,))
      print("  Image: %r" % (the_puzzle,))
      print("  Image: %r" % (link,))
      break

    the_puzzle = link

  if the_puzzle == None:
    print("No puzzle found on %r" % (url,))
    return

  image_path = download_file(the_puzzle)
  date_str = puzzle_match.group(1).replace("/","-")
  info = (date_str, image_path)
  info_set.add(info)

def process_list_page(info, n, count):
  done = set()
  url = "%s%d" % (killer_list_base, n)
  for link in iter_elems(url, "a", "href"):
    if not link.startswith(site):
      continue

    suffix = link[len(site):]

    should_ignore = False
    for r in ignore_re_list:
      if r.fullmatch(suffix):
        should_ignore = True
    if should_ignore:
      continue

    puzzle_match = False
    for r in puzzle_page_re_list:
      m = r.fullmatch(suffix)
      if m:
        puzzle_match = m
    if not puzzle_match:
      print("Unknown link on list page: '%s'\n" % (link,))
      continue

    if link in done:
      continue
    done.add(link)

    if count > 0:
      process_puzzle_page(info, link, puzzle_match)
      count -= 1

def main():
  try:
    os.mkdir(cache_dir)
  except FileExistsError:
    pass
  info_set = set()
  process_list_page(info_set, 29, 5)
  for info in sorted(info_set):
    print(info)

main()
sys.exit(0)
