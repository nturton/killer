#! /usr/bin/python3
import hashlib
import html.parser
import json
import os
import os.path
import sys
import re
import requests

base_dir = os.path.dirname(sys.argv[0])
cache_dir = os.path.join(base_dir, "cache")
index_path = os.path.join(cache_dir, "index.json")

special_char_re = re.compile(r'[^\w\.]+')

site = "https://www.theguardian.com/lifeandstyle/"
killer_list_base = site + "series/killer-sudoku+series/sudoku?page="
ignore_re_list = (
  re.compile(r'[-\w]+'),
  re.compile(r'series/killer-sudoku\+series/sudoku\?page=\d+'),
  re.compile(r'series/killer-sudoku\+series/sudoku'),
)
puzzle_page_re_list = (
  re.compile(r'(\d+/[^\W\d]+/\d+)/observer-killer-sudoku'),
  re.compile(r'(\d+/[^\W\d]+/\d+)/sudoku-killer-\d+'),
)
image_ignore_re_list = (
  re.compile(r'//phar.gu-web.net/.*'),
  re.compile(r'https://sb.scorecardresearch.com/.*'),
)
puzzle_image_re_list = (
  re.compile(r'https://i\.guim\.co\.uk/img/static/sys-images/Guardian/Pix/pictures/'
             '(\d+/\d+/\d+)/[-/\w]+.jpeg\?.*'),
  re.compile(r'https://i\.guim\.co\.uk/img/media/\w+/[\w_]+/[\w]+/\d+.jpg?.*'),
)
extn_re = re.compile(r'.*(\.\w+)(\?|\Z)')

chunk_size = 4096

months = dict(x.split(" ",1) for x in (
  "jan 1",  "feb 2",  "mar 3",  "apr 4",  "may 5",  "jun 6",
  "jul 7",  "aug 8",  "sep 9",  "oct 10", "nov 11", "dec 12",
))

def url_filename(url):
  m = extn_re.match(url)
  extn = ""
  if m:
    extn = m.group(1)

  h = hashlib.sha256()
  h.update(url.encode("UTF-8"))
  dig = h.digest()
  return dig.hex()+extn

def download_file(index, url, descr):
  filename = url_filename(url)
  path = os.path.join(cache_dir, filename)
  tmp_file = path + ".tmp"

  print("Downloading %r" % (url,))

  index[filename] = {
    'descr': descr,
    'url': url,
  }

  if os.path.exists(path):
    print("  Local: %r (cached)" % (path,))
    return path

  f = open(tmp_file, "wb")
  r = requests.get(url, stream=True)
  for data in r.iter_content(chunk_size=chunk_size):
    f.write(data)
  f.close()
  os.rename(tmp_file, path)

  print("  Local: %r (fetched)" % (path,))
  return path

class link_handler(html.parser.HTMLParser):
  def __init__(self, tag, attr_name):
    super().__init__()
    self.__tag = tag
    self.__attr_name = attr_name
    self.__links = []

  def __iter__(self):
    return iter(self.__links)

  def handle_starttag(self, tag, attrs):
    if tag != self.__tag:
      return

    href = dict(attrs).get(self.__attr_name)
    if href == None:
      return

    self.__links.append(href)

def iter_elems(path, tag, attr_name):
  h = link_handler(tag, attr_name)
  f = open(path, "r")
  while True:
    data = f.read(chunk_size)
    if len(data) == 0:
      break
    h.feed(data)
  f.close()

  for link in h:
    yield link

def process_puzzle_page(index, url, date_str):
  print("Puzzle page: %r" % (url,))

  the_puzzle = None
  path = download_file(index, url, "page "+date_str)
  for link in iter_elems(path, "img", "src"):
    should_ignore = False
    for r in image_ignore_re_list:
      if r.fullmatch(link):
        should_ignore = True
    if should_ignore:
      continue

    is_puzzle = False
    for r in puzzle_image_re_list:
      m = r.fullmatch(link)
      if m:
        is_puzzle = True

    if not is_puzzle:
      print("Unknown image on %r" % (url,))
      print("  Image: %r" % (link,))
      continue

    if the_puzzle != None:
      print("Duplicate puzzle found on %r" % (url,))
      print("  Image: %r" % (the_puzzle,))
      print("  Image: %r" % (link,))
      break

    the_puzzle = link

  if the_puzzle == None:
    print("No puzzle found on %r" % (url,))
    return

  image_path = download_file(index, the_puzzle, "image "+date_str)

def process_list_page(index, n, count):
  done = set()
  url = "%s%d" % (killer_list_base, n)
  path = download_file(index, url, "list %d" % n)
  for link in iter_elems(path, "a", "href"):
    if not link.startswith(site):
      continue

    suffix = link[len(site):]

    should_ignore = False
    for r in ignore_re_list:
      if r.fullmatch(suffix):
        should_ignore = True
    if should_ignore:
      continue

    puzzle_match = False
    for r in puzzle_page_re_list:
      m = r.fullmatch(suffix)
      if m:
        puzzle_match = m
    if not puzzle_match:
      print("Unknown link on list page: '%s'\n" % (link,))
      continue

    if link in done:
      continue
    done.add(link)

    if count > 0:
      date_str = puzzle_match.group(1).replace("/","-")
      process_puzzle_page(index, link, date_str)
      count -= 1

def load_index():
  if os.path.exists(index_path):
    f = open(index_path)
    index = json.load(f)
    f.close()
  else:
    index = {}
  return index

def save_index(index):
  tmp_path = index_path + ".tmp"
  f = open(tmp_path, "w")
  json.dump(index, f, indent=2)
  f.close()
  os.rename(tmp_path, index_path)

def main():
  try:
    os.mkdir(cache_dir)
  except FileExistsError:
    pass

  index = load_index()
  for n in range(24, 30):
    process_list_page(index, n, 50)
  save_index(index)

main()
sys.exit(0)
